# **Project Implementation Plan: TunnelForge (Ngrok-like Tunneling Service (MVP))**

This document outlines a detailed plan to develop an open-source npm package that replicates **ngrok-like HTTP/HTTPS tunneling** with added **rate limiting** and a **React dashboard** for monitoring. The goal is to build a minimal viable product (MVP) in under two weeks as a solo developer, using Node.js (TypeScript) for the backend and React for the frontend, with the project hosted on GitHub as a portfolio piece.

## **Implementation Status**

Current Progress:
- ‚úÖ Phase 1: Planning & Requirements complete
- ‚úÖ Phase 2: Project Setup & Environment complete
- üîÑ Phase 3: Core Backend Implementation in progress
  - Basic Express server setup complete
  - WebSocket infrastructure in place
  - API router stub created
  - CLI arguments handling implemented
- üîÑ Phase 4: React Dashboard Implementation started
  - Basic Vite + React setup complete
  - Initial App component created
  - Proxy configuration done
- ‚è≥ Phase 5: Testing & Stabilization pending
- ‚è≥ Phase 6: Deployment & Documentation pending

Next Steps:
- Implement actual tunnel functionality
- Complete WebSocket protocol for tunnel connections
- Add request forwarding logic
- Implement rate limiting
- Develop dashboard UI components
- Create real API endpoints

Ngrok is a popular tool that creates a secure tunnel from a public URL to a local server, allowing anyone to access a developer's local application via the internet. Open-source alternatives like **localtunnel** similarly expose a localhost service to the world with minimal setup. This project will implement a similar tunneling service as an npm package, featuring built-in request rate limiting for abuse prevention and a web dashboard (built with React) for monitoring tunnels and traffic (much like ngrok's web UI at [http://localhost:4040](http://localhost:4040/) that inspects HTTP traffic).

Below is the comprehensive plan, structured by development phases, module checklists, testing strategy, and technical specifications.

## **Phase-Wise Breakdown**

The project is divided into phases with specific tasks and sub-tasks in each. This phased approach ensures focus on core functionality first, followed by enhancements and polishing within the 2-week timeline.

### **Phase 1: Planning & Requirements (Day 1‚Äì2)**

* **Define Core Features:** Document the primary functionality ‚Äì tunneling HTTP/HTTPS traffic from a local port to a public URL, similar to ngrok. Identify enhancements: request **rate limiting** and a **web dashboard** for monitoring.

* **Research & Feasibility:** Review how existing tools work (ngrok, localtunnel). For example, localtunnel uses a client-server model where the client opens a TCP connection to a server which then forwards HTTP requests to the client's local server. Decide on using a similar reverse proxy approach (Node server as the tunnel server and a Node client/CLI connecting from local).

* **Technology Selection:** Confirm the tech stack:

  * Backend with **Node.js** and **TypeScript** (for type safety).

  * Use **Express** (Node.js web framework) for HTTP handling and APIs.

  * Use **WebSockets** or **Node's net module** for the tunneling connection between client and server.

  * Frontend with **React** (likely via Create React App or Vite) for the dashboard.

  * Choose libraries for ease: e.g., **express-rate-limit** for implementing rate limiting.

* **Scope & MVP Cuts:** Given the 2-week limit, prioritize a single-user, single-tunnel scenario initially. Defer non-essential features (e.g., persistent subdomains or advanced auth) to future iterations. The MVP will focus on one tunnel at a time with basic monitoring.

### **Phase 2: Project Setup & Environment (Day 2‚Äì3)**

* **Repository Initialization:** Create a new GitHub repository. Set up two main folders in the project (monorepo style):

  * `/server` for backend (Node.js \+ TypeScript).

  * `/dashboard` for frontend (React app).

* **Node.js Project Configuration:** Initialize `package.json` in root (for npm package) and within `/server` if treating server as publishable component. Configure TypeScript in the backend (`tsconfig.json`).

* **Baseline Dependencies:** Install essential packages:

  * Backend: `express`, `ws` (WebSocket) or `socket.io` (for real-time tunnel connections), `express-rate-limit` (rate limiting middleware), `http-proxy` or native `http` for request forwarding if needed.

  * Frontend: `react`, `react-dom`, possibly UI library (or simple HTML/CSS for dashboard), plus tooling like `vite` or `create-react-app`.

* **Development Tools:** Set up Linters/Formatters (ESLint, Prettier) for code quality. Configure **GitHub Actions** (if time permits) for CI to run tests and linters on pushes/PRs.

* **Scaffold Project Structure:** Create initial folder layout (see **Project Structure** section for details) and stub out key files:

  * Backend: index or server entry point (e.g., `server/src/index.ts`), with a basic Express server listening on a port and a placeholder for tunnel handling.

  * Frontend: use `create-react-app` or similar to scaffold a React project in `/dashboard`, verifying it starts with a sample page.

### **Phase 3: Core Backend Implementation (Day 4‚Äì7)**

*This phase focuses on building the tunneling server, client (CLI), and rate limiting logic on the backend.*

* **Tunneling Server (Backend):** Implement a basic **tunnel server** that can accept connections from a local client and forward HTTP traffic:

  * Create an Express server to listen for incoming **public HTTP requests** (this will act as the entry point for external traffic).

  * Design a mechanism to distinguish traffic for different tunnels. In a full production scenario, this is done via subdomains (e.g., `<random>.domain.com` mapped to a specific client). For MVP (without custom DNS), use one of:

    * **Unique URL path:** e.g., `http://server:3000/<tunnelId>/...` where `<tunnelId>` is a unique identifier for the tunnel. The server parses the URL and routes the request to the correct client.

    * **Unique port per tunnel:** Alternatively, when a client connects, the server opens a new ephemeral port (or uses a worker Express server) for that tunnel. The client is given a public URL with that port. (This avoids needing path parsing or DNS; e.g., `http://<server-host>:port` as the tunnel endpoint).

  * Use **WebSockets** or a persistent TCP connection between server and client:

    * When the client (running on the user's machine) connects to the server (perhaps to an endpoint like `/connect` via WebSocket), the server assigns an ID or port for that tunnel and keeps the connection open.

    * For each incoming HTTP request on the server intended for that tunnel, forward the request data through the socket to the client. The client will then proxy it to the local target server (the developer's local app) and return the response, which the server then sends back to the original requester.

    * Implement basic request forwarding logic. This can be done by sending raw HTTP over the socket or by a simpler approach: use an HTTP proxy library. For MVP, a straightforward method is to handle HTTP requests on the server by buffering the request (method, headers, body), sending a message to the client, then waiting for the client's response to respond to the HTTP caller.

  * **Rate Limiting:** Integrate the `express-rate-limit` middleware to throttle incoming requests per tunnel:

    * For example, limit to *N* requests per minute per IP or per tunnel. This prevents abuse by excessive calls. Attach the rate limiter to the Express routes serving tunnel traffic. The middleware will automatically respond with HTTP 429 if the rate is exceeded.

    * Configure rate limit parameters (windowMs, max requests) suitable for demonstration (e.g., 100 requests/minute per tunnel).

    * Ensure that if multiple tunnels are supported, rate limiting is scoped appropriately (likely by IP or by a key including the tunnel ID).

  * **CLI Client (Local agent):** Implement a CLI (command-line interface) tool that developers will use to connect their local server to the tunnel server:

    * Use Node.js to create a CLI script (e.g., `bin/tunnel.js`) that can be installed via npm. This CLI will accept arguments like `--port 8080` (the local port to expose) and possibly `--host` (the tunnel server address if self-hosting).

    * When run, the CLI will establish a connection to the tunnel server (websocket or tcp) and register the tunnel. It sends the desired local port and perhaps a requested subdomain or ID.

    * The CLI then listens on a local port for instructions or directly proxies connections:

      * Option 1: **Pull model** ‚Äì The CLI keeps open 1 or more connections to the server. Upon an incoming request, the server signals the CLI, which then makes an HTTP request to the local app and returns the response.

      * Option 2: **Push model** ‚Äì The CLI could itself open a server on the local machine and accept connections from the main server (if network allows), but simpler is the pull model above.

    * For MVP simplicity, implement a single persistent connection to handle all traffic sequentially. (As an enhancement, multiple parallel connections could be opened to handle concurrent requests, similar to how localtunnel opens 10 connections for throughput, but this can be skipped initially.)

    * Provide user feedback: once connected, output the public URL (either `http://<server>:<port>/<id>` or `http://<id>.<domain>` if domain is configured). For example, "Tunnel established at \*\*[http://localhost:3000/abcd\*\*](http://localhost:3000/abcd**%E2%80%9D).

    * Handle client-side errors gracefully (e.g., connection drop, local server down, etc.) possibly with auto-reconnect or at least clear messages.

* **Basic Authentication (Optional):** If time permits, implement a simple token or key to ensure only authorized clients connect to the server (since open-source, maybe not needed for local use, but good practice).

* **Internal API for Dashboard:** On the server, set up an API endpoint (e.g., `GET /api/tunnels`) that returns information about active tunnels and their stats (tunnel ID, target local port, number of requests handled, etc.). This will be consumed by the React dashboard. Also consider an endpoint for recent requests or logs if feasible (e.g., `GET /api/tunnels/{id}/logs`).

  * Maintain in-memory tracking of each tunnel: start time, total requests, last request timestamp, and whether rate limit has been hit.

  * Update these stats in the middleware or request handler so they can be reported to the dashboard.

* **Logging:** (Lightweight) Implement console logging or a debug log for tunnel events (connections, requests) to assist in testing and provide transparency in development.

### **Phase 4: React Dashboard Implementation (Day 8‚Äì10)**

*In this phase, build the React frontend that will serve as a dashboard to monitor the tunnels.*

* **Dashboard Setup:** Initialize the React project (if not done in Phase 2). If using Create React App, ensure it's ejected or configured for customization as needed. Alternatively, use Vite for a quick TypeScript React setup.

* **UI Design:** Plan the dashboard's main screen:

  * A table or list of active tunnels, showing fields like **Tunnel ID/URL**, **Local Port**, **Status** (connected/disconnected), **Requests Count**, **Rate Limit Status** (e.g., requests remaining in window).

  * Possibly a section for **Recent Requests** per tunnel (listing last few requests with timestamp, path, status).

  * If multiple tunnels can be open (even in future), design list to handle multiple; if only one at a time in MVP, the UI can still show a single entry but structured for scalability.

  * Include an area to show any global messages or errors (e.g., if the server is unreachable).

* **State Management:** Use React hooks and state to fetch and display data:

  * On component mount, call the backend API (e.g., `fetch('/api/tunnels')`) to get the current tunnels info. Polling could be used (e.g., update every 5 seconds) or use WebSocket subscription if the server sends real-time updates (the latter could be complex to add; polling is simpler for MVP).

  * Display the fetched data in the UI. For example, show "Tunnel ID: abcd1234 | Port: 5000 | Requests: 45 | Last Request: 2s ago".

  * If including recent request logs: provide an expand view per tunnel to show the last N request details (method, URL path, response status, time).

* **Interactivity:** (Optional for MVP) Provide controls:

  * A button to **close a tunnel** (which would call a DELETE `/api/tunnels/{id}` endpoint on the server to shut it down).

  * A button or form to **open a new tunnel** from the dashboard (this would require the server to initiate a client connection or the client to be running in a mode to accept commands, which is advanced; likely skip in MVP).

  * If not controlling tunnels, the dashboard is primarily read-only in MVP, which is acceptable.

* **Styling:** Keep the UI minimal but clean. Use a simple CSS framework or custom CSS for layout:

  * Possibly use **Ant Design** or **Chakra UI** for a quick table and cards (only if time permits learning curve; otherwise, basic HTML table and form).

  * Ensure the dashboard is responsive enough and clearly readable.

* **Integration with Backend:** Handle CORS or proxy:

  * During development, configure the React dev server to proxy API requests to the Node server (e.g., if React runs on port 3001 and Node on 3000, proxy `/api` to `http://localhost:3000`).

  * For production build, plan to serve the React app via the Node server (e.g., build React and have Express serve the static files, perhaps on a separate route or the root). This integration can be done in Phase 5\.

* **Testing during Dev:** Manually run the backend and frontend to ensure the dashboard correctly displays information from a running tunnel. This real-time manual test is important before formal testing phase:

  * Simulate a running tunnel (run the CLI for a local port) and generate some requests (open a browser to the public URL). Confirm the dashboard updates the request count and shows the new requests.

### **Phase 5: Testing & Stabilization (Day 11‚Äì12)**

* **Write Unit Tests (Backend):** Create unit tests for critical functions:

  * e.g., test the function that generates or validates tunnel IDs, test that the rate limiter configuration is set up correctly (maybe using a small timeframe for test), test any utility functions (like parsing messages or formatting data for the dashboard).

  * Use a testing framework like **Jest** for JavaScript/TypeScript on the backend.

  * If possible, mock the networking: e.g., simulate a fake client connection to ensure the server's request routing logic calls the right functions.

* **Write Integration Tests (Backend):** Test end-to-end tunnel flow in a controlled environment:

  * Launch a test HTTP server (serving dummy content) on a local port (simulate the user's local service).

  * Programmatically start the tunnel server (perhaps in a child process or in-memory if the design allows).

  * Run the tunnel client (maybe as a function if exported, or also as a subprocess) pointing to the dummy local server.

  * Use a library like **supertest** or **axios** to send an HTTP request to the tunnel's public URL (could be `http://localhost:<port>/<id>/...` in test).

  * Verify that the response matches the dummy server's response, indicating the tunnel worked correctly.

  * Also verify the rate limiting: e.g., send more than allowed requests in a short time and expect a 429 Too Many Requests response for the excess ones.

* **Test React Components:** Use **React Testing Library** and Jest to test the dashboard's components in isolation:

  * Mock the fetch API to simulate `/api/tunnels` responses and ensure the UI renders the tunnel list correctly.

  * Test that after updating state (simulating new data), the new values appear on screen.

  * If any complex logic in hooks, test that separately (possibly by extracting logic to pure functions if needed).

* **Integration Test for Dashboard (optional):** If time permits, run the whole system and use a tool like **Cypress** or **Puppeteer** for an end-to-end test:

  * Start backend and a simulated tunnel, serve the production build of the dashboard, and automate a browser to check that the data is visible. This might be heavy for MVP, so treat as stretch goal.

* **Bug Fixing:** As tests run, fix any failures. Also do exploratory testing:

  * Run the CLI and server in various scenarios (what if local server is down? What if server starts after client? etc.) to handle edge cases gracefully.

  * Check for memory leaks or crashes on repeated use.

* **Performance Check:** Ensure that the basic implementation can handle at least a moderate load (e.g., test with \~50 concurrent requests if possible). This is not for scalability now, but to ensure no immediate bottlenecks or crashes with multiple requests.

* **Documentation Drafting:** Begin writing usage documentation (README) concurrently, so that any issues in the usage flow are caught early and documentation stays up-to-date with implementation.

### **Phase 6: Deployment & Documentation (Day 13‚Äì14)**

* **Prepare for Release:** Finalize the npm package details:

  * Ensure the root `package.json` has correct name, version, bin field (for CLI), description, keywords (to highlight "ngrok-like", "tunneling", etc.), author info, and license.

  * Run a build for the backend (TypeScript compile to JavaScript in a `dist` folder).

  * Build the React dashboard for production (`npm run build` in the dashboard project) and ensure the static files are included or referenced in the server. Possibly copy the build output into `server/public` and have Express serve it.

  * Verify that the package can be installed and used (e.g., using `npm pack` and testing the tarball locally).

* **GitHub Repository Polishing:**

  * Write a comprehensive **README.md** with:

    * Project description and badges (CI status, license, etc.).

    * Installation instructions (`npm install -g <package>` or usage via npx).

    * Usage examples (how to start the server, how to run the CLI to open a tunnel, expected output).

    * Features list (tunneling, rate limiting, dashboard) and any limitations of the MVP.

    * Screenshots of the running dashboard (if possible) to showcase the UI.

  * Create or update **CHANGELOG.md** with initial release notes.

  * Ensure repository has proper **LICENSE** (e.g., MIT).

  * Set up repository topics/tags on GitHub for visibility (e.g., "tunneling", "ngrok", "nodejs").

* **GitHub Pages (Optional):** If desired, publish the documentation or demo page via GitHub Pages, or host the dashboard there. (This could also be done by making the dashboard a static site, but since our dashboard is tied to the running server for data, GH Pages might only host static documentation or screenshots.)

* **Final Review & Commit:** Do a final code review (even as a solo developer, go through diff or use a pretend Pull Request to self-review). Clean up any debug code, ensure all TODOs are resolved or moved to future tasks.

* **Deployment:** Since this is a portfolio project:

  * Optionally **publish the package to npm** (this makes it truly an npm package others can install). If doing so, double-check version and test one more install from npm registry (perhaps after release).

  * If not publishing, ensure the GitHub repo is public and well-structured to be reviewed by others.

  * Tag the release (e.g., v0.1.0) in Git and push.

* **Post-MVP Plans:** Document any features or improvements that are left out due to time (such as custom subdomain support, authentication, more concurrent tunnels, persistent storage, etc.), to show a roadmap for the project's future.

## **SDLC Checklist by Module**

Below is a **Software Development Life Cycle (SDLC)**-based checklist for each major module of the project, covering stages from requirements to deployment. This serves as a progress tracker and ensures each component has completed all development stages.

### **Module 1: Core Tunneling Engine (Backend Server & Client)**

* **Requirements:** (‚úÖ *Defined*) Identify functionalities for establishing tunnels, forwarding HTTP/HTTPS traffic, and unique URL assignment for each tunnel. Confirm that the module will handle single-client connections initially, with placeholders for multi-tunnel support.

* **Design:** (‚úÖ *Completed*) Outline how the server and client communicate (WebSocket vs TCP, message formats), how the server will map incoming requests to tunnels (using path or port mappings), and how rate limiting integrates into the request flow. Design includes module interface: e.g., a `TunnelServer` class with methods to start/stop and a `TunnelClient` class/CLI to initiate connection.

* **Implementation:** (üîÑ *In Progress*) Code the server in TypeScript (Express setup, WebSocket handling, request proxy logic) and the CLI client (argument parsing, connection setup, local proxying). Ensure the server properly spawns per-tunnel handlers or routes.

* **Testing:** (üîÑ *Pending*) Write unit tests for internal functions (like ID generation, message handling) and integration tests for end-to-end tunneling (server-client loop). Verify rate limit triggers by sending bursts of requests.

* **Deployment:** (üîÑ *Pending*) Integrate this module into the npm package distribution (include CLI in package `bin`, ensure server code is compiled to JS and included). Confirm that running `npx <tool> --port 8080` effectively starts the server (if required) and client or otherwise running separate commands as documented.

### **Module 2: React Dashboard (Frontend)**

* **Requirements:** (‚úÖ *Defined*) Determine the data to display (active tunnels, stats, logs) and interactions (viewing data, possibly closing tunnels). Ensure the dashboard refreshes data periodically or via push.

* **Design:** (‚úÖ *Completed*) Create wireframes or sketches of the UI ‚Äì a main dashboard page listing tunnels, and maybe a detail view for each. Decide on using a single-page app with React Router (likely not needed for MVP) or just a single view. Plan component structure: e.g., `<TunnelList>`, `<TunnelStatsCard>`, etc., and state management (React hooks, context if needed).

* **Implementation:** (üîÑ *In Progress*) Build the React components and pages. Implement API calls to the backend (`fetch('/api/tunnels')`) and state updates. Apply styling for usability. Ensure production build outputs static files that can be served by the backend.

* **Testing:** (üîÑ *Pending*) Write unit tests for components (using mocked data). Ensure components render correctly with given props or state. Run a development instance connected to a real backend to test manual scenarios (tunnel comes and goes, data updates). Check that the dashboard handles no-tunnel (empty state) gracefully.

* **Deployment:** (üîÑ *Pending*) Build the app for production. Update the backend to serve the static files (e.g., an Express static middleware pointing to `dashboard/build`). Verify that navigating to the dashboard (e.g., `http://localhost:4040` if chosen port 4040\) shows the UI and it can fetch data from the running server. Include instructions in documentation for how to start the dashboard (it might be automatic when server runs, or a separate `npm start` for dev mode).

### **Module 3: Rate Limiting Feature**

*(This is a cross-cutting concern in the backend, but listed separately to ensure its development cycle is tracked.)*

* **Requirements:** (‚úÖ *Defined*) The system must prevent excessive usage by limiting the number of requests through a tunnel per unit time. Decide on the limit (e.g., 100 requests/min per client IP or per tunnel). It should inform the client or user when the limit is reached (HTTP 429 responses).

* **Design:** (‚úÖ *Completed*) Choose middleware-based implementation using `express-rate-limit` for simplicity. Determine if a global rate limit or a per-tunnel instance is needed. Design config: window size, max hits, and the message or behavior when exceeded. Plan where to attach this in the Express request handling pipeline (likely on the router serving tunneled requests).

* **Implementation:** (üîÑ *In Progress*) Install and configure **express-rate-limit**. Initialize it with the chosen policy (e.g., `windowMs = 60*1000ms, max = 100` for 100 requests/minute per IP by default). Apply it to the relevant Express route (or globally to all incoming tunnel requests). Ensure the middleware is only affecting external requests, not the internal API.

* **Testing:** (üîÑ *Pending*) Unit test the configuration (e.g., using a small `windowMs` in a test environment and sending dummy requests to trigger it). Integration test by making more requests than allowed through the tunnel and expecting a 429 response for the overflow. Verify that normal usage under the limit is unaffected. Confirm headers like `Retry-After` or RateLimit headers if any are correct (the middleware can send standard rate-limit headers for transparency).

* **Deployment:** (üîÑ *Pending*) No special deployment step aside from including it in the server. Ensure documentation mentions the rate limiting feature and the defaults. If the rate limits are configurable via environment or parameters, document those.

### **Module 4: GitHub Repository & DevOps**

*(Covers version control, CI, and general repo maintenance as a "module" of the project.)*

* **Requirements:** (‚úÖ *Defined*) Maintain code in a GitHub repository with clear history. Enforce good practices for branch management, commits, and possibly continuous integration for testing.

* **Design:** (‚úÖ *Completed*) Decide on branch strategy: e.g., use a `main` branch for stable code and feature branches for development. Plan to use **Conventional Commits** format for commit messages (e.g., "feat: add tunnel server logic") for clarity. Outline a simple PR workflow even as a solo dev (self-review via PRs before merging to main). Determine CI needs: use GitHub Actions to run tests and linters on push.

* **Implementation:** (üîÑ *In Progress*) Set up branch protection rules on `main` (if applicable). Create initial `develop` or feature branches as needed (e.g., `feature/core-tunnel`, `feature/dashboard-ui`). Write a `.github/workflows/ci.yml` for running Node tests and React build on each commit. Implement commit hooks (using Husky) for linting on commit or commit message lint if using commitlint with Conventional Commits.

* **Testing:** (üîÑ *Pending*) Verify that CI passes with the current test suite. Test that merging a PR triggers the workflow. Ensure that the repo is accessible and the README instructions actually work on a fresh clone. (In lieu of a team code review, consider using a static analysis tool or performing a thorough code walkthrough.)

* **Deployment:** (üîÑ *Pending*) Tag the repository with a release. If publishing to npm, test the `npm publish` process on a dry run. Ensure the GitHub repo has the **GitHub Releases** updated (create a release description pointing to the tag and listing features). Optionally, post on relevant forums or communities (Dev.to, Reddit r/node) to showcase the project as part of portfolio deployment.

## **Module-Wise Testing Plan**

Testing will be conducted at both **unit** and **integration** levels for each major module. Below is a breakdown of the testing strategy for each module:

### **1\. Tunneling Engine (Server & Client) Tests**

* **Unit Tests (Backend Server):**

  * *Tunnel ID Generation:* If the server generates tunnel identifiers (for subdomains or URL paths), test that the IDs are unique and conform to expected format/length.

  * *Request Routing Logic:* Abstract the core request forwarding function (e.g., a function that takes an incoming HTTP request object and the target client connection) and test it with a mock connection. Use dummy data to ensure that the function correctly packages the request data for the client.

  * *Utility Functions:* Test any helper utilities (for example, a function that parses messages received from the client or composes an HTTP response from client data).

  * *Rate Limit Config:* If using a custom configuration, test that the `express-rate-limit` middleware is initialized with the right options (perhaps by inspecting the `handler` or by simulating rapid calls to a protected endpoint and checking that further calls are blocked).

* **Unit Tests (Client CLI):**

  * *Argument Parsing:* If the CLI uses a library like yargs or commander, test that providing various flags (`--port`, `--host`, etc.) results in the correct configuration internally.

  * *Connection Handling:* Factor out the client's connection logic into a function (e.g., `connectToServer(host, port)`) and test its behavior. For instance, simulate a server that immediately closes connection or sends an unexpected response, and verify the client handles it (perhaps by retrying or outputting an error).

  * *Local Proxy Function:* If the client has a function that receives a request from the server and then proxies it to the local app, test this function with a stubbed local server response to ensure it formats the response back properly.

* **Integration Tests (End-to-End Tunnel):**

  * Set up a **dummy local HTTP server** within the test (using Node's http module or Express) that listens on a random port and returns a known response (e.g., "Hello Tunnel") for a test endpoint.

  * Programmatically start the tunnel server (perhaps by requiring the main server module and calling an init function in a child process or separate thread if needed to mimic real operation).

  * Use the tunnel client code to connect to the server and point it at the dummy local server's port. (This could be done by invoking the CLI with `child_process.spawn` or by calling the underlying client logic if accessible via an API).

  * Once the tunnel is established (the client should log or the server should indicate readiness), use an HTTP client (like Axios or supertest) to send a request to the **public URL** provided by the server. For example, if the server indicated the tunnel URL is `http://localhost:3000/abcd`, the test would GET `http://localhost:3000/abcd/test-endpoint`.

  * Expect the response to match exactly what the dummy local server returns. This validates that the request traveled through the tunnel and back.

  * Test edge cases:

    * **Tunnel Down:** Stop the client process and then attempt a request to the public URL, expecting either a specific error message or no response.

    * **Large Payload:** Send a larger payload (if POST is supported in MVP) through the tunnel to ensure the data is transmitted correctly (could be a text blob or JSON).

    * **Concurrent Requests:** If possible, fire multiple requests asynchronously to see if the single-connection tunnel queues them appropriately or if any are dropped. (With one connection, they might queue ‚Äì the test should verify that all requests eventually get a response).

  * **Rate Limit Integration:** In the same end-to-end test, perform more than the allowed number of requests in a minute:

    * For instance, if limit is 5 requests per second for test, send 6 quick requests. The expected outcome is that the 6th response is an HTTP 429 status with possibly a message like "Too Many Requests".

    * Verify the first 5 succeeded and only the 6th (and subsequent within that window) are blocked.

    * Also verify that after waiting for the window reset, requests are accepted again.

  * Clean up by shutting down the tunnel server and client after tests (to avoid port blocking for subsequent tests).

### **2\. React Dashboard Tests**

* **Unit Tests (Components & Utils):**

  * *Tunnel List Component:* If there is a component that renders the list/table of tunnels, test it by feeding in sample data. For example, provide a prop with an array of tunnel objects (with made-up values for id, port, count, etc.) and assert that the DOM contains those values (using React Testing Library's queries).

  * *Stats Display Logic:* If there is logic to calculate rate limit usage (e.g., percent of quota used), put that in a pure function and test it with various inputs (0% usage, 50% usage, over 100%).

  * *API Hook/Context:* If using a custom React hook for data fetching (e.g., `useTunnelsData` that fetches from `/api/tunnels`), mock the fetch call (using `jest.spyOn(global, 'fetch')`) to return a known JSON, then invoke the hook in a test component (using React Testing Library's `renderHook`) to ensure it processes data correctly (e.g., sets loading state, then data state).

  * *Individual Components:* Test presentational components like a Tunnel card or list item rendering all fields correctly and calling any callback props (if e.g., a "Close" button triggers a prop function, simulate a click and verify it calls).

* **Integration Tests (Dashboard \<-\> API):**

  * Use a tool like **Jest \+ msw (Mock Service Worker)** to simulate the backend API while running component tests:

    * Set up MSW to intercept calls to `/api/tunnels` and respond with predefined JSON (e.g., one tunnel with id "abcd", port 5000, count 10, etc.).

    * Render the dashboard page component (which will cause it to fetch the API). Assert that after a short wait, the page displays the data from the mocked API.

    * This verifies that the data fetching and state update loop is working and the UI updates accordingly.

  * (Optional) **End-to-End UI Test:** If time allows, use **Cypress** to run the actual app:

    * Start the real backend server with a running tunnel (or modify it to have a dummy mode where one fake tunnel is present).

    * Start the React dev server (or use the built app served by the backend).

    * Use Cypress to visit the dashboard URL, and check that the page shows the expected tunnel info. Simulate user clicking a "Close Tunnel" button if implemented and verify the tunnel entry disappears or an API call was made (this might require stubbing the API in Cypress or having a real effect).

    * This kind of test ensures that the front and back integrate correctly in a running environment.

* **Usability Checks:** Not automated, but manually verify in browser:

  * The dashboard should update reasonably quickly when a new tunnel is created or an existing one closed. (Manually start a second tunnel if supported or restart the existing one and see if the UI refreshes.)

  * Check the dashboard in different browsers/screen sizes for responsiveness.

  * Verify that all text, numbers, and any time displays are formatted clearly.

### **3\. Combined System Tests**

* **Simultaneous Module Integration Test:** Run a scenario where all parts are active:

  * Launch the tunnel server, open a tunnel with the CLI, and have the dashboard open in a browser. Perform an action (like making requests through the tunnel) and observe the dashboard updating in real-time (or near real-time).

  * While not easily automated, this manual integrated test is crucial. For example, start the CLI for a local port, then open the dashboard in a browser to see the new tunnel appear. Then navigate to the public URL to generate traffic, and watch the dashboard increment the request count.

  * If feasible, automate parts of this: e.g., use a headless browser to load the dashboard and an HTTP client to hit the tunnel, then have the test verify via the dashboard's API (maybe query the DOM or call the same internal API) that counts increased.

* **Performance Test (Basic):** Using a tool like **Apache Bench (ab)** or \*\* autocannon\*\* for Node, send a burst of requests through the tunnel and monitor for any crashes or slowdowns. This can reveal if the single-thread Node process is handling I/O well (Node is generally good with I/O, but our code must not block event loop).

* **Security Test (Basic):** Attempt some misuse:

  * Try to send an invalid formatted request through the tunnel (maybe telnet to the port and send gibberish) and ensure the server handles it (likely by dropping the connection but staying alive).

  * Ensure that the dashboard API is not exposed beyond what's needed (for instance, the dashboard endpoints should ideally be separate from the tunneled traffic endpoints to avoid any possibility of conflicts or unauthorized access. Check that one cannot accidentally fetch `/api/tunnels` from the internet-facing tunnel endpoint without proper route separation).

  * Check that sensitive operations (if any, like closing tunnels) are protected (for MVP, if there's no auth, it's okay, but note this in docs as a potential risk if someone finds the local dashboard).

The testing plan above strives for a balance between thoroughness and the time constraints of an MVP. Emphasis is on critical path (tunnel data integrity, rate limiting enforcement, and correct dashboard display). Non-essential tests (like exhaustive UI interactions or large-scale performance) are noted as future improvements.

## **Technical Requirements Document**

This section specifies the technical details of the project, including the chosen languages, frameworks, project structure, dependencies, environment setup, testing tools, version control standards, and deployment instructions. It serves as a blueprint for development and a reference for anyone setting up or reviewing the project.

### **Language and Framework Versions**

* **Node.js (Backend):** The project will use **Node.js v18.x LTS** (or later) with **TypeScript**. Node 18+ is chosen for its stable features and support for ES modules, as well as built-in APIs that might be useful (like `tls` or `http2` if needed in future). TypeScript will be at version **4.x** (ensuring compatibility with Node 18 and modern syntax). This combination provides a robust development environment with type-checking and modern JavaScript features.

* **Express.js:** Express **4.x** (latest 4.x release) will be used as the web framework on the Node side. Express is a minimalist framework suitable for setting up HTTP endpoints easily and has broad middleware support (needed for things like logging and rate-limiting).

* **WebSocket Library:** If using raw WebSockets for the tunnel connection, the **`ws`** library version 8.x will be used (a popular lightweight WebSocket implementation for Node). Alternatively, if the design uses **Socket.io**, then version 4.x of socket.io server and client would be used (though this might be overkill for a simple tunnel, it's an option for ease of use).

* **React (Frontend):** Use **React 18.x** (latest) for building the dashboard, along with **React DOM 18.x**. This ensures we have the latest features (hooks, concurrent rendering if needed, etc.). The build tool could be **Create React App 5** (which supports React 18\) or **Vite 4** for a faster, simpler setup with TypeScript support.

* **JavaScript/TypeScript Target:** Transpile to ES2019+ for backend (Node 18 supports a lot of ES2020 features natively, so the TS config can target a relatively high level or even ESNext and rely on Node's capability). For React, use JSX/TSX with target ES2015+ since bundlers will handle compatibility.

* **Other Tools:**

  * Dev environment will rely on **ts-node** (for running TypeScript directly in development for backend) and possibly **nodemon** for auto-restarting the server on code changes.

  * Package manager: **npm 9** (bundled with Node 18\) or Yarn/Pnpm depending on developer preference (npm is default unless specified).

  * **Browser Support:** The dashboard being a developer tool can target modern browsers (latest Chrome/Firefox/Edge) without polyfills for old IE, etc., to simplify development.

### **Project Structure and Layout**

The repository is organized to separate the backend (Node.js server and CLI) and frontend (React dashboard) while allowing them to work together. Below is the proposed folder structure:

```
root/
‚îú‚îÄ‚îÄ server/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.ts          # Entry point for the tunnel server
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tunnelServer.ts   # Core server class or logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tunnelClient.ts   # (Optional) If client logic is included here, otherwise in separate CLI file
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/              # Express route handlers for dashboard API (e.g., tunnels list, logs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/            # Utility modules (e.g., id generation, logging helpers)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __tests__/        # Unit tests for server code
‚îÇ   ‚îú‚îÄ‚îÄ bin/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ connect.js        # CLI executable (compiled from TS, this will be linked as the npm bin)
‚îÇ   ‚îú‚îÄ‚îÄ package.json          # Separate package config for server (if treating independently, else use root package.json)
‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.json         # TypeScript config for backend
‚îÇ   ‚îî‚îÄ‚îÄ README.md             # Maybe separate README for server usage (but main README in root covers it)
‚îú‚îÄ‚îÄ dashboard/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx           # React root component
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/       # React components (TunnelList, TunnelCard, etc.)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/            # If using route pages (maybe only one Dashboard page)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/              # API utility (e.g., functions or hooks to call backend API)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __tests__/        # Unit tests for React components
‚îÇ   ‚îú‚îÄ‚îÄ public/               # Static public assets for React (if needed)
‚îÇ   ‚îú‚îÄ‚îÄ package.json          # Separate for frontend if not using monorepo tooling
‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.json         # TS config for React
‚îÇ   ‚îî‚îÄ‚îÄ README.md             # Could have a README for how to run dashboard standalone (optional)
‚îú‚îÄ‚îÄ package.json              # Root package config for the npm package (listing dependencies, bin entry, etc.)
‚îú‚îÄ‚îÄ package-lock.json         # Lockfile for reproducible builds
‚îú‚îÄ‚îÄ README.md                 # Primary README with project overview and instructions
‚îú‚îÄ‚îÄ CHANGELOG.md              # Changelog tracking changes per version
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .eslintrc.js              # ESLint configuration
‚îú‚îÄ‚îÄ .prettierrc               # Prettier configuration
‚îî‚îÄ‚îÄ .github/
    ‚îú‚îÄ‚îÄ workflows/ci.yml      # CI pipeline definition
    ‚îî‚îÄ‚îÄ ISSUE_TEMPLATE/       # (Optional) GitHub issue templates

```
**Notes on structure:**

* The **`server/src`** directory contains the main logic. We separate it into logical modules:

  * `tunnelServer.ts` could contain a class or set of functions to start the server, accept connections, and manage tunnels.

  * `tunnelClient.ts` may contain a class to represent a connected client (though in MVP, we might not need a full class, the client is the CLI itself).

  * Alternatively, the client code might live in the CLI file under `server/bin/connect.ts` (compiled to `connect.js`), since it largely orchestrates connecting to server and forwarding traffic.

  * `api/` subfolder holds route handlers for the dashboard API (e.g., `getTunnels.ts` for GET /api/tunnels).

  * `utils/` for shared helpers (like a custom logger, a function to format bytes, etc.).

* The **`dashboard/src`** has typical React structure with `components` and possibly `pages`. Given a simple app, all logic might reside in `App.tsx` and a few components.

* We keep separate `package.json` in `server` and `dashboard` for clarity during dev (especially if using different build steps or dev dependencies). However, for simplicity of building the final npm package, the root `package.json` will include the production dependencies of both. The CLI bin entry (e.g., `"bin": {"mytunnel": "server/bin/connect.js"}`) is defined in root package.json so that when installed, `mytunnel` command is available.

* TypeScript configs are separate to tailor compilation for Node vs browser.

* Tests are co-located in `__tests__` directories (or could use naming \*.spec.ts). We may also have a top-level `tests/` folder for integration tests that spin up both server and client; e.g., `tests/e2e.test.ts` that is run with a special setup (perhaps not under server or dashboard but separate).

* Other configs: `.eslintrc.js` and `.prettierrc` at root to enforce coding standards across both backend and frontend.

* `.github/workflows` contains CI definitions (like running `npm run build && npm run test`).

* We might use a monorepo tool (like npm workspaces or lerna) to manage server/dashboard together. This can be configured in root package.json with `"workspaces": ["server", "dashboard"]`. This allows running e.g., `npm run build` in root that triggers building both. If not using workspaces, we will manage them manually with scripts.

### **Package Dependencies**

Below are the key libraries and dependencies for the project:

* **Backend Dependencies:**

  * **express** ‚Äì Fast, unopinionated web framework for Node.js to handle HTTP endpoints (both for tunnel and API).

  * **ws** ‚Äì WebSocket library to create a WebSocket server and client for tunnel communication (lightweight and fits our needs to send messages between server and client).

  * **express-rate-limit** ‚Äì Middleware to apply rate limiting on Express routes (to fulfill the rate limiting requirement).

  * **yargs** or **commander** ‚Äì For parsing command-line arguments in the CLI. This makes it easy to define flags like `--port` or `--host`.

  * **chalk** (optional) ‚Äì For coloring CLI output (to make the terminal messages more readable or highlight errors).

  * **debug** (optional) ‚Äì A logging library to enable conditional debug output. Can be useful to trace tunneling activity when needed without always printing to console.

  * **http-proxy** (optional) ‚Äì If decided to use a proxy approach, this library can forward HTTP requests to a target. However, in our design we likely handle forwarding manually via the socket, so this might not be needed.

  * **cors** (maybe not needed)\*\* ‚Äì The dashboard calls the API on the same server (if served together) so CORS issues might not occur. If during dev the React dev server calls the Node server, enabling CORS on the Node API or using the CRA proxy is needed. We can use the `cors` package to allow localhost dev origins if needed.

* **Frontend Dependencies:**

  * **React** and **React-DOM** ‚Äì Core for building UI.

  * **axios** or **fetch API** ‚Äì For making HTTP requests to the backend. We can use the built-in fetch since modern browsers support it (no need for Axios unless we want its conveniences).

  * **Chart.js or similar** (optional) ‚Äì If we want to show a chart of request rates. Likely not for MVP, but worth noting if metrics visualization was desired.

  * **Material-UI / Ant Design / Chakra UI** (optional) ‚Äì A component library for quickly styling the dashboard. Given time constraints, using a ready UI kit can speed up development. If chosen, list it as a dependency (e.g., Chakra UI which is a simple and popular React component library).

* **Dev Dependencies:**

  * **TypeScript** ‚Äì for type checking and transpilation (targeting ES6 for Node and ES5 for React by default via Babel).

  * **ts-node** ‚Äì for running TS scripts directly (useful in dev/testing for the server).

  * **nodemon** ‚Äì for auto-restarting the server on file changes in development.

  * **Jest** ‚Äì testing framework for both backend and possibly frontend (CRA comes with Jest by default for React).

  * **ts-jest** ‚Äì to run TypeScript tests smoothly with Jest.

  * **React Testing Library** ‚Äì for unit testing React components.

  * **MSW (Mock Service Worker)** ‚Äì for mocking API calls in frontend tests.

  * **ESLint** \+ **typescript-eslint** ‚Äì for linting TypeScript code.

  * **Prettier** ‚Äì for consistent code formatting.

  * **Husky** ‚Äì for git hooks, e.g., to run linting or tests pre-commit.

  * **Commitlint** ‚Äì if adopting Conventional Commits, to enforce commit message format via a git hook.

  * **Cypress** (optional) ‚Äì for end-to-end testing with the real application in a browser.

* **Peer Dependencies:** None expected for an end-user (the npm package will bundle or list needed deps; users just install it).

* **Note:** All dependencies should be as updated as possible (given this is a portfolio project in 2025, using latest stable versions is preferred to showcase up-to-date practices).

### **Development Environment Setup**

To set up the development environment for this project, follow these steps:

1. **Prerequisites:** Ensure **Node.js (\>=18.x)** is installed on your machine. It is recommended to use a Node version manager (nvm) to match the project's Node version. Also install **npm 8/9** (comes with Node 18\) or your choice of Yarn/Pnpm if using those (the project by default will assume npm).

2. **Clone Repository:** `git clone https://github.com/<yourusername>/<repo-name>.git` and `cd <repo-name>`.

3. **Install Dependencies:** Run `npm install` at the root. This will install both backend and frontend dependencies if using npm workspaces. If not using workspaces and managing separately, do:

   * `cd server && npm install`

   * `cd ../dashboard && npm install`  
      so that both subprojects get their dependencies.

4. **Environment Variables:** The project might use a `.env` file for configuration. Create a `.env` in the root or in `server/` with any required settings. For example:

   * `TUNNEL_SERVER_PORT=3000` (port on which the tunnel server will run)

   * `DASHBOARD_PORT=4040` (if serving dashboard on a separate port or separate dev server)

   * `RATE_LIMIT_WINDOW_MS=60000` and `RATE_LIMIT_MAX=100` to tweak rate limiting (optional).  
      These will be loaded by something like dotenv in development. Document each variable in a `.env.example` file.

5. **Running in Development (Backend):**

   * Go to `server/` and run `npm run dev`. This might be set up to use ts-node or nodemon to compile and run `src/index.ts`. The server will start (by default on port 3000 or as configured).

   * The server dev mode should log to console when it's ready, e.g. "Tunnel server listening on port 3000".

   * Also, running the server in dev mode might automatically serve the React app's static files if they are built, but during active development, we'll use React's dev server.

6. **Running in Development (Frontend):**

   * Open a new terminal, go to `dashboard/` and run `npm start` (if Create React App) or `npm run dev` (if Vite). This starts the development server for the React app (commonly on port 3000 for CRA, but if 3000 is taken by backend, it might use 3001 or another port ‚Äì it will indicate which).

   * If needed, allow the dev server to proxy API requests: in CRA, set `"proxy": "http://localhost:3000"` in `dashboard/package.json` so that calls to `/api/*` are forwarded to the Node server. In Vite, configure `proxy` in `vite.config.js` similarly.

   * The React app will auto-open in the browser (for CRA) or you can navigate to `http://localhost:3000` (or the port it shows) to view the dashboard. Initially it may show "No active tunnels" if none are running.

7. **Linking Backend and Frontend:**

   * When both dev servers are running, test the integration: Start a tunnel by running the CLI manually: e.g., `node server/bin/connect.js --port 8080`. This should connect to the dev server and print a URL (likely [http://localhost:3000/](http://localhost:3000/)).

   * Make a request to that URL (open in browser or use curl) and see if the dummy traffic shows up on the dashboard (which should query the backend for stats). In dev, there could be separate origins (dashboard on 3001 calling API on 3000); ensure CORS is handled or proxy is working.

8. **Running Tests:**

   * **Backend tests:** Run `npm test` in `server/` (configured to run Jest on `.test.ts` files). Ensure any server not in use is torn down properly in tests to free ports.

   * **Frontend tests:** Run `npm test` in `dashboard/` (CRA uses `react-scripts test` which opens an interactive watch mode by default; press `a` to run all tests once or configure CI mode).

   * For end-to-end tests (if configured), there might be a script like `npm run test:e2e` at root which expects the servers to be running or will start them in the background and then run Cypress.

   * Check coverage reports if needed with `npm run coverage` (depending on scripts configured for Jest with coverage).

9. **Building for Production:**

   * To build the backend, run `npm run build` in `server/` ‚Äì this triggers TypeScript compilation to output JavaScript (likely in `server/dist`).

   * To build the dashboard, run `npm run build` in `dashboard/` ‚Äì outputs static files in `dashboard/build`.

   * The root might have a script to build both (if using workspaces or a root-level script that calls both).

   * After building, test running the production version: `node server/dist/index.js` (assuming index.ts becomes index.js) which should start the server. The server should serve the dashboard static files (if integrated). Then run the CLI (which now would use the compiled connect.js from `dist` or from `bin`, depending on how we set the build). Make sure everything still works in production mode.

10. **Troubleshooting Setup:**

    * If the CLI is not recognized, ensure you ran `npm install -g .` in the project root to link the package globally in dev, or use `npx ts-node server/src/index.ts` (or whichever file runs the server or client).

    * If the dashboard won't fetch data in dev, check CORS settings or proxy config (common pitfall: forgetting to proxy `/api`).

    * If port conflicts occur (both CRA and server want 3000), change one ‚Äì e.g., run server on 4000 by setting env and update proxy, or accept CRA switching to 3001 automatically.

By following the above, a developer should be able to get the environment running and iteratively develop and test the application.

### **Testing Tools and Coverage Goals**

* **Testing Framework:** We use **Jest** as the primary testing framework for both backend and frontend. Jest provides a unified way to write tests and assertions, and it's configured out-of-the-box for React projects. On the backend, Jest plus ts-jest will handle TypeScript seamlessly.

* **Assertion Library:** The assertions will be done via Jest's built-in expect API. For any specialty assertions (like on DOM nodes), React Testing Library extends Jest with DOM matchers.

* **React Testing:** **@testing-library/react** is included to facilitate rendering components in a test DOM and making assertions on their output. It encourages testing the app from the user's perspective (finding elements by text, role, etc., rather than testing implementation details).

* **Mocking:** For modules like network requests:

  * Use Jest's mocking capabilities or **Mock Service Worker (msw)** to simulate API responses in frontend tests.

  * On backend, use Jest to mock out external calls or heavy dependencies if any (though our backend might not need to call external services in this project).

* **Integration Testing:** For higher-level integration or E2E, consider:

  * **supertest** for hitting the Express server endpoints directly in test.

  * **Cypress** for running a live server and interacting with it via a headless browser. If Cypress is too heavy for initial MVP, rely on manual testing and possibly add Cypress later.

* **Coverage:** Aim for **80% or higher code coverage** on critical modules. Given the tight schedule, 100% coverage is not realistic, but focus on core logic:

  * Tunneling mechanism (server/client) ‚Äì high coverage on message handling and routing.

  * Rate limiting ‚Äì tests to ensure limits trigger.

  * React dashboard ‚Äì at least the rendering of main components and one round-trip data fetch should be covered.

  * Exclude trivial code (index.ts that just starts server, or auto-generated CRA files) from coverage metrics so they don't skew results.

* Configure coverage collection in Jest (using `--coverage` flag or Jest config). Optionally generate an HTML coverage report for inspection.

* **Continuous Integration:** Integrate tests into the GitHub Actions workflow. E.g., a job that installs dependencies, runs `npm run build` and then `npm test -- --coverage` to ensure tests pass and to report coverage. (The coverage can be checked to ensure it doesn't drop below a threshold).

* **Manual Testing:** In addition to automated tests, allocate time for manual testing sessions:

  * Cross-platform manual test: run the CLI on Windows, Mac, Linux if possible (Node being cross-platform should work, but things like how signals or certain networking behaves might differ).

  * If domain or subdomain is tested, maybe manually configure a hosts file or a dummy DNS using something like `xip.io` or `nip.io` to simulate subdomain routing to localhost, and test that scenario.

  * Browser compatibility: open the dashboard in at least two different browsers (Chrome and Firefox, for example) to catch any possible issues (should be minimal if using standard React and Fetch APIs).

By using these tools and approaches, we ensure the project is not only working but also maintainable, with a safety net of tests for future changes.

### **GitHub Repository Standards**

Maintaining high standards in the GitHub repository is important for collaboration (even if solo, it demonstrates professionalism) and for portfolio quality. The following practices will be followed:

* **Branching Strategy:**

  * The repository will have a permanent **main** (or **master**) branch which always holds stable, tested code.

  * Feature development will occur in separate branches named descriptively. Use a naming convention such as `[type]/[short-description]`, where **type** could be `feature` for new features, `fix` for bug fixes, `chore` for non-functional updates, etc.. For example: `feature/tunnel-server-basic` or `feature/react-dashboard-ui`. This makes it clear what each branch is for.

  * If multiple features are being developed in parallel (less likely for a solo dev in 2 weeks), multiple branches can exist and eventually be merged.

  * Optionally use a **develop** branch as an integration branch if not merging directly to main until a release. But given the short timeline, merging feature branches into main after review is sufficient.

* **Commit Message Convention:** Adopt the **Conventional Commits** specification to keep commit history readable and structured. This means commit messages will have a prefix like `feat:`, `fix:`, `docs:`, `refactor:`, etc., followed by a short description:

  * e.g., `feat: implement basic tunnel server` or `fix: handle null socket error on client disconnect`.

  * This style not only makes the log uniform but also helps in automatically generating changelogs or determining version bumps if using semantic versioning.

  * Each commit should ideally relate to one logical change. Avoid huge commits that mix unrelated changes.

* **Pull Requests:** Even though the developer is solo, all changes should go through a Pull Request on GitHub:

  * Open a PR from the feature branch to main with a clear description of what's being added/changed.

  * The PR template (if added in `.github/PULL_REQUEST_TEMPLATE.md`) can remind to check tests passing and coverage.

  * Before merging, ensure CI checks (tests, lint) are green. The developer can perform a self-review, adding comments if any parts need explanation for a future viewer.

  * Use **squash merge** or **rebase merge** to keep main history clean (squash if many WIP commits are in the branch, or rebase for a linear history preserving individual commits).

  * PRs should reference any issues if created (though for MVP the developer may not need to formalize issues for each feature, but could create GitHub issues for major tasks to track progress).

* **Issue Tracking:** Use GitHub Issues to track bugs or future enhancements. Tag issues appropriately (bug, enhancement, question). Although solo, this practice helps demonstrate project management skills. It also helps if others star or fork the project and report issues.

* **Code Reviews:** If possible, have a peer or friend review some code via the PR, but if not, the solo dev should do a self-review checklist:

  * Check for any obvious bugs or console.log debug statements left.

  * Ensure all user-facing strings are correct and clear.

  * Evaluate if any refactoring is needed for readability.

  * Ensure tests cover the new changes (if not, add before merging).

* **GitHub Actions CI:** Set up at least one action workflow (named e.g., `ci.yml`) that runs on push and PR:

  * Install deps, run lint, run tests, perhaps build. This ensures that any push to main or PR cannot accidentally break the build without the developer noticing.

  * Optionally add a status badge in the README for the CI build status.

* **Releases and Tags:** Use Git tags to mark released versions (e.g., `v0.1.0`). If publishing to npm, these should correspond to the npm versions. Consider using GitHub Releases to write release notes (which can largely mirror the CHANGELOG entries).

* **Community Health Files:** Add files like `CODE_OF_CONDUCT.md` and `CONTRIBUTING.md` to outline expectations if others want to contribute. In a portfolio project, these show professionalism. In CONTRIBUTING, mention how to run the project and tests, and the commit/PR guidelines.

* **Branch Protection:** Enable branch protection on main: require PRs before merge, and require status checks (CI) to pass. This prevents even the solo dev from accidentally pushing directly to main without tests.

* **Commit Signing:** If desired, sign commits or tags (optional, but some projects do this for authenticity).

* **Licensing:** The project will be under MIT License (as common for open source). Ensure the LICENSE file is present and referenced in package.json.

* **README Content:** Ensure the README (and any additional docs) stay updated with any changes, especially if command usage or installation steps change during development.

By adhering to these standards, the project will be easier to maintain, and its repository will look professional and be inviting to other developers or recruiters inspecting it.

### **Local Deployment and Usage Instructions**

This section provides concise instructions on how to deploy (run) the MVP on a local machine and use it, as it would appear in the README for users. For the MVP, "deployment" is about running the tool locally (since it's not a cloud service yet), and verifying it works end-to-end.

**Prerequisites:** Node.js 18+ and npm installed.

**Installation:** The package can be installed globally (if published to npm) or run via npx. For example, after publishing:

```
npm install -g my-tunnel-tool   # assuming "my-tunnel-tool" is the package name
```

*(If not published, a user can clone the repo and run `npm install` then `npm run build` to get the compiled code.)*

**Starting the Tunnel Server:** In the MVP, the tunnel server may start automatically when you run the CLI to open a tunnel. For simplicity of use, the CLI could both start the server (possibly in the background) and connect, especially if it's all one package running locally. However, a more transparent approach is:

Open a terminal and run the server:

 mytunnel-server \--port 3000

1.  This would start the tunneling server on port 3000 (accessible at `http://localhost:3000`). The server will listen for tunnel clients and also serve the dashboard (on `http://localhost:3000/dashboard` or similar).

   * Alternatively, run via npx: `npx my-tunnel-tool server --port 3000`.

   * The server prints a message like "Tunnel server running on port 3000".

2. In a real scenario, you'd run the server on a machine with a public IP or domain. For local testing, localhost is fine.

**Connecting a Tunnel (CLI Client):** From another terminal (on the same machine for local testing):

mytunnel connect \--port 8080

This will:

* Connect to the tunnel server (assuming default localhost:3000 or if server host is specified via a `--host` argument).

* Register a tunnel for your local port 8080\. Your local web service on 8080 (could be an app or just a test server) will be exposed.

The CLI will output a public URL. For example:

```
Tunnel established! 
Forwarding: http://localhost:3000/abcd -> http://localhost:8080
```

*  This indicates that any request to `http://localhost:3000/abcd` will be tunneled to your local `http://localhost:8080`.

* (If the server was running on a remote host with a domain, it might output something like `https://abcd.tunnel-domain.com -> http://localhost:8080`.)

Keep this CLI running; it maintains the connection. You might see logs of any requests it forwards.

**Accessing the Public URL:** Now, from a browser or an API client, you (or anyone) can access the public URL. For example, open `http://localhost:3000/abcd` in a browser. You should see the content served by your local server on port 8080, as if it were coming from the tunnel server.

* You can share this URL with others on your network (or the internet, if your tunnel server is public) for them to access your local app.

**Rate Limiting Behavior:** If a user makes too many requests too quickly (beyond the set threshold, e.g., 100 requests/min), the server will start returning **HTTP 429 Too Many Requests** for subsequent calls. In the browser, this might appear as an error page or a message "Too Many Requests". In the CLI logs, you might see a note that rate limit was hit. This is to prevent abuse; you can wait a short while and try again.

**Using the Dashboard:** The React dashboard provides a visual interface to monitor the tunnels:

* Open `http://localhost:3000` in your web browser (or a specific dashboard URL if designated, e.g., `http://localhost:3000/dashboard` or perhaps the root is the dashboard when accessed via a browser).

* The dashboard will display the active tunnel (or tunnels). For each tunnel, you can see:

  * The **Tunnel ID/URL** (like `abcd` in the example).

  * The local port it's connected to (8080).

  * How many requests have gone through.

  * Rate limit info (e.g., "20/100 requests this minute" or time until reset).

  * Perhaps a timestamp of last activity.

* The dashboard might update live (or you can refresh periodically) to see new requests count.

* If implemented, you could click a "Close" button to disconnect a tunnel from the dashboard, which would stop forwarding (the CLI would probably exit when the server instructs it to close).

**Local Stop/Teardown:**

* To stop the tunnel, simply press **Ctrl+C** in the CLI terminal. This will disconnect the client. The server will detect this and free up that tunnel ID.

* To stop the server, press **Ctrl+C** in the server terminal. This will shut down the tunnel server and the dashboard. All active tunnels (if any) will be closed.

**Reconnecting:** You can restart the CLI or server anytime. Tunnel IDs are generated new each time by default (unless a feature to request a specific ID/subdomain is provided via an option).

**Example Use Case:**  
 Suppose you have a local development server running on [http://localhost:8080](http://localhost:8080/) with a web app. You want to show it to a colleague. You would:

* Start `mytunnel-server` (if not already running).

* Run `mytunnel connect --port 8080` and get a URL like `http://<your-ip>:3000/abcd` (if on same network, your colleague can use your IP).

* Share that URL; your colleague visits it and sees your web app.

* Monitor the dashboard to watch the requests as they interact. If they hammer it and go above rate limit, they'll get blocked temporarily (which you'll also see indicated on the dashboard).

**Additional Notes:**

* If the package is installed globally, the commands `mytunnel-server` and `mytunnel connect` (or possibly a single `mytunnel` command with subcommands) will be available. If using via npx, prefix commands with `npx`.

* On first run, the system might prompt firewall permissions (especially on Windows or Mac) since it's opening ports for listening.

* The tool currently does not enforce authentication ‚Äì meaning anyone who knows your tunnel URL can access it. For secure sharing, you might want to add your own app's auth or only share URLs privately. Future versions could incorporate access control.

* If the tunnel server is deployed on a cloud VM or similar, you would run `mytunnel-server` on the VM, and on your local machine run `mytunnel connect --host <vm-address> --port 80 --local-port 8080` (for example) to connect to it. Ensure ports are open and DNS is configured if using hostnames.

These instructions ensure a user (or the developer themselves) can run and test the MVP. All commands and behaviors described would be verified during development. They will be refined in the README to avoid ambiguity and to troubleshoot common issues (e.g., "If you get EADDRINUSE, the port is busy ‚Äì use `--port` to specify a different port for the server").

---

**References:**

* Localtunnel (open-source ngrok alternative) exposes your localhost to the world for easy testing and uses a client-server architecture with subdomain routing.

* Ngrok provides a web interface for inspecting traffic (e.g., via [http://localhost:4040):contentReference\[oaicite:22\]{index=22}](http://localhost:4040\):contentreference[oaicite:22%5D%7Bindex=22%7D/), inspiring the React dashboard for this project.

* Rate limiting is implemented using Express middleware to control the rate of incoming requests, protecting the tunnel from abuse.

* Git branching and commit conventions follow widely adopted practices to maintain a clean project history.

